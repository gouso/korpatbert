{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 임포트\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "from korpat_tokenizer import Tokenizer\n",
    "import tensorflow as tf\n",
    "import bert\n",
    "import os\n",
    "import mecab\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./korpat_bert_test_classification_model.h5\",custom_objects={\"BertModelLayer\": bert.BertModelLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#분류하고자 하는 파일 설정\n",
    "dataset_path = \"./classification_test.csv\"              # 분류를 원하시는 dataset의 경로를 넣어주시면 됩니다.\n",
    "\n",
    "# 필요 환경변수 설정\n",
    "os.environ['TF_KERAS'] = '1'    # Keras Tensorflow 설정\n",
    "config_path     = \"./pretrained/korpat_bert_config.json\" # KorpatBert Config 파일 경로\n",
    "vocab_path      = \"./pretrained/korpat_vocab.txt\"        # KorpatTokenizer Vocabulary 파일 경로\n",
    "checkpoint_path = \"./pretrained/model.ckpt-381250\"       # KorpatBert 모델파일 경로\n",
    "pretreind_model_dir = \"./pretrained/\"                    # KorpatBert 모델 디렉토리 경로\n",
    "\n",
    "\n",
    "MAX_SEQ_LEN = 512 # 학습 최대 토큰 갯수\n",
    "\n",
    "\n",
    "# 데이터 셋 로드 함수\n",
    "# 입력 : dataset_url (전체 데이터셋 경로)\n",
    "# 출력 : test_data(평가데이터)\n",
    "def dataset_load(dataset_url):\n",
    "    test_data = pd.read_csv(dataset_url,encoding='cp949')\n",
    "    return test_data\n",
    "\n",
    "# 학습을 위한 데이터 셋 전처리 함수\n",
    "# 입력 : dataset(전처리 대상 데이터셋)\n",
    "# 출력 : tokens(토큰화 결과), x_data(입력데이터), y_data(정답데이터)\n",
    "def preprocessing_dataset(dataset):\n",
    "    tokens, indices, labels = [], [],[]\n",
    "    \n",
    "    # 데이터셋의 문장을 토큰화 및 인코딩 처리하고, 라벨을 One hot 벡터 변환으로 처리한다.\n",
    "    for sentence, label in tqdm(zip(dataset['sentence'],dataset['label']), desc = \"데이터 전처리 진행중\"):\n",
    "        tokens.append(tokenizer.tokenize(sentence))\n",
    "        ids, _ = tokenizer.encode(sentence, max_len=MAX_SEQ_LEN)\n",
    "        indices.append(ids)\n",
    "    x_data = np.array(indices)\n",
    "        \n",
    "    return tokens, x_data\n",
    "\n",
    "\n",
    "# 메인 프로그램 시작\n",
    "if __name__ == '__main__':\n",
    "    # KorPat Tokenier 선언\n",
    "    tokenizer = Tokenizer(vocab_path=vocab_path, cased=True)\n",
    "\n",
    "    # 필요 데이터셋 로드\n",
    "    test_data = dataset_load(dataset_path)\n",
    "    print(\"\\n\\n===> 평가데이터 샘플 출력 및 전처리 시작 <===\")\n",
    "    print(test_data[:10])\n",
    "    test_tokens, test_x  = preprocessing_dataset(test_data)\n",
    "    print(\"test_x\",test_x)\n",
    "    print(\"test_x\",len(test_x))   \n",
    "      \n",
    "    # 라벨의 크기 정의\n",
    "    # 모델 저장 및 테스트 데이터를 이용한 평가\n",
    "    predict = model.predict(test_x) #id \n",
    "    print(\"text_x예측embedding\",predict)\n",
    "    preds_flat = np.argmax(predict, axis=1).flatten()\n",
    "    print(\"예측값\",preds_flat)\n",
    "    print(\"text_x예측개수\",len(preds_flat))\n",
    "    #모델이 예측한 값을 저장하다.\n",
    "       \n",
    "    print('\\n===> 분류가 완료 되었습니다')\n",
    "    #df=pd.concat([test_data['sentence'],preds_flat])\n",
    "    df=pd.DataFrame({'원문sentence':test_data['sentence'],'예측값':preds_flat})\n",
    "    df.to_csv(f\"./classification_results.csv\",encoding='cp949')      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
